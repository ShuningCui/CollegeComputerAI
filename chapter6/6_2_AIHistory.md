---
marp: true
theme: MyGaia
paginate: true
---

<!-- _class: lead -->
## 人工智能的发展

---
### **人工智能的孕育期（1943-1955）**

+ 现在公认的人工智能的第一项工作是由 Warren McCulloch 和 Walter Pitts(1943) 完成的。受 Pitts 的顾问 Nicolas (1936, 1938)的数学建模工作的启发，综合利用了三个资源：大脑中神经元的基本生理学和功能知识；罗素（Russell）和怀特黑德（Whitehead）对命题逻辑的形式分析；和图灵的计算理论。
+ 他们提出了一种人工神经元模型，其中每个神经元都被表征为"开"或"关"，并可以响应足够数量的相邻神经元的刺激而切换到"开"。神经元的状态被认为是"等同于提出其足够刺激的命题"。例如，他们表明，任何可计算的函数都可以通过一些连接的神经元网络来计算，并且所有的逻辑连接（AND、OR、NOT        等）都可以通过简单的网络结构来实现。

---

### **人工智能的孕育期（1943-1955）**
哈佛大学的两名本科生 Marvin Minsky (1927-2016) 和 Dean Edmonds 于 1950年建造了第一台神经网络计算机。曼彻斯特大学的 Christopher Strachey 和 IBM的 rthur Samuel 在 1952 年独立开发了两个跳棋程序。
而艾伦·图灵是最有影响力的。早在 1947年，他就在伦敦数学学会就该主题进行了演讲，并在 1950年发表了文章"计算机与智能"，文章中介绍了图灵测试，机器学习、遗传算法和强化学习。

---

### **人工智能诞生（1956）**

>1956年，当时这个领域非常有影响力的约翰.麦肯锡说服了明斯基、香农等人，帮助他把全美所有自动机理论、神经网络和智能研究的人召集到了一起，1956夏天，他们在达特茅斯组织了一场研讨会，从这场会议的声明中可以看出那时候的科学家对人工智能持何种乐观态度：

---
### **人工智能诞生（1956）**

>我们提议1956年夏天在新罕布什尔州汉诺威市的达特茅斯大学开展一次由10个人组成、为期两个月的人工智能研讨会。研究智能的每个方面、以及任何其他可被这样精确地描述的特征原则，以至于能够建造一台机器来模拟它。该研究将基于这个推断来进行，并尝试着发现如何使机器能够使用语言，形成抽象与概念，求解多种现在注定由人来求解的问题，进而改进机器。我们认为：如果仔细选择一组科学家对这些问题一起工作一个夏天，那么对其中的一个或多个问题就能够取得意义重大的进展。

---
### **人工智能诞生（1956）**
>那时候的顶级科学家其实希望迅速地做完图灵对计算机所做的事情，但很不幸，事情的进展和他们想的完全不一样，并且可以确定时至今日，这事也还没完成。但这次会议的特别价值在于它形成了一种共识，即让人工智能成为一个独立的学科，因此这个会议通常被看成是人工智能这一学科真正诞生的标志。

---

### **人工智能的第一次发展（1956-1974）**

### *乐观派*
>人工智能是在人们信心大爆棚时诞生的，尽管科学家非常乐观，也声称自己的程序能够证明《数学原理》第2章中的大部分定理，但大多数人并不能从这一乐观态度中看到什么明显的进步。当时美国政府对此非常热心，在这个领域投了很多钱：1963年6月，麻省理工学院从新成立的高级研究计划署（后称为DARPA）获得了220万美元的资助。这笔钱被用于资助MAC项目，该项目囊括了Minsky和McCarthy五年前创建的"AI Group"。直到70年代，DARPA每年仍然提供300万美元。

---

### **人工智能的第一次发展（1956-1974）**
### *悲观派*
与之相反英国政府却采取了一种完全不同的做法，他们请了一位著名的数学家------詹姆斯·莱特希尔（Sir James Lighthill）教授，对人工智能做一个彻底的评估。这位教授在看了所有重要的相关论文后，写出了一份报告，后来世人称之为《莱特希尔报告》。这份报告说人工智能绝不可能有什么用途，因为它只能被用来解决简单的问题。英国政府以后没有在人工智能上进行大量的投资，此后人工智能逐渐变得少有人问津。

---

### **人工智能的第一次发展（1956-1974）**
期望越高，失望越大。虽然人工智能领域在诞生之初的成果层出不穷，但还是难以满足社会对这个领域不切实际的期待。由于先驱科学家们的乐观估计一直无法实现，从70年代开始，对人工智能的批评越来越多。人工智能的第一次发展止步于以下三种困难：

（1）早期的人工智能程序对句子的真实含义完全不理解，它们主要依赖于句法处理获得成功。也就是说，不理解主题含义，简单的基于不同语言之间的句法转化，再根据电子词典进行单词的替换。在英俄互译中，"the
spirit is willing but the flesh is waek"（心有余而力不足）被翻译为了"the
vodka is good but the meat is
rotten（伏特加是好的，而肉是烂的）。其实直到现在问题仍然存在，只不过大量的数据弥补了不理解真实含义的缺陷。形象地讲，现在计算机并不去理解这个句子，而是看哪种翻译被用得多。

（2）第二个是《菜特希尔报告》里强调的组合爆炸。大部分早期AI程序求解问题的方式是尝试各种可能步骤的不同组合。这种方法在问题规模很小的时候是奏效的，然而随着问题规模的增加，每个步骤上都有无数的岔路甚至岔路间还彼此勾连，因此可能的步骤近乎无限多，那么这个方法就变得毫无价值。在计算复杂性理论发展起来之前，普遍认为解决一个规模变大的问题只是需要增加内存和更快的硬件就可以实现。实际情况则是程序原则上能找到解并不等同于实际中可以找到解。

（3）第三个是那时候发现虽然人工智能具有的神经网络简单形式可以学会他们表示的任何东西，但它其实只能表示很少的东西，应用范围十分有限。

正因为这些困难得不到有效的解决，随着公众热情的消退和投资的大幅度削减，人工智能在20世纪70年代中期进入了第一个冬天。

### 6.2.4 人工智能的第二次发展（1980-至今）

进入80年代，由于专家系统和人工神经网络等技术的新进展，人工智能的浪潮再度兴起。到了80年代确实有些专家系统被成功部署，并为公司节约了数以千万美元计的费用，比如第一个成功的商用专家系统R1在DEC成功运转，此后DEC陆续部署了40个专家系统。也正是在这时候日本宣布了第五代计算机计划，希望用10年时间研制出智能计算机。作为回应，美国也组建了一家公司来保证国家竞争力。

也是在这个时候，神经网络上取得了新的进展，一个典型的事件是1989年，AT＆T
Bell实验室验证了一个反向传播算法在现实世界中的杰出应用，\"反向传播应用于手写邮编识别系统，即该系统能精准识别各类手写数字\"，很有意思的是当年的演示视频被保留了下来，所以我们今天仍然可以清楚地回放当年的效果。但很不幸的，展开这类算法所需要的计算能力和数据那时候并不具备，所以在实际应用中也败下阵来。这个方向狼狈到这样一种程度：现在的深度学习领军人物以及他们学生的论文被拒成了家常便饭，根本原因就是论文主题是神经网络。另一件小事也可以从侧面说明当时神经网络不被待见的程度：为了让神经网络复兴并被大家接受，现在鼎鼎大名的杰弗里.欣顿（Geoffrey
Hinton）和它的小组密谋用"深度学习"来重新命名让人闻之色变的神经网络领域。很多人很难想到今天鼎鼎大名的深度学习其实是这么来的。

到了90年代，研究人工智能的学者开始引入不同学科的数学工具，比如高等代数、概率统计与优化理论，这为人工智能打造了更坚实的数学基础。数学语言的广泛运用，打开了人工智能和其他学科交流合作的渠道，也使得成果能得到更为严谨的检验。在数学的驱动下，一大批新的数学模型和算法被发展起来，比如，统计学习理论、支持向量机、概率图模型等。新发展的智能算法被逐步应用于解决实际问题，比如安防监控、语音识别、网页搜索、购物推荐和自动化算法交易等等。新算法在具体场景的成功应用，让科学家们看到了人工智能再度兴起的曙光。

进入了21世纪，全球化的加速以及互联网的蓬勃发展带来全球范围电子数据的爆炸性增长。人类迈入了"大数据"时代。在数据和计算能力指数式增长的支持下，人工智能算法也取得了重大突破。2006年，Hinton在神经网络的深度学习领域取得突破，人类又一次看到机器赶超人类的希望，也是标志性的技术进步。在2012年一次全球范围的图像识别算法竞赛ILSVRC（也称为Image
Net挑战赛）中，多伦多大学开发的一个多层神经网络Alex
Net取得了冠军，并大幅度超越了使用传统机器学习算法的第二名。这次比赛的成果在人工智能学界引起了广泛的震动。从此，以多层神经网络为基础的深度学习被推广到多个应用领域，在语音识别、图像分析、视频理解等诸多领域取得成功。2016年，谷歌通过深度学习训练的阿尔法狗（AlphaGo）程序在一场举世瞩目的比赛中以4比1战胜了曾经的围棋世界冠军李世石，它的改进版更在2017年战胜了当时世界排名第一的中国棋手柯洁。这一系列让世人震惊的成就再一次点燃了全世界对人工智能的热情。世界各国的政府和商业机构都纷纷把人工智能列为未来发展战略的重要部分。

## 6.3人工智能研究领域与典型问题

### 6.3.1 人工智能主要研究领域

现在一谈论人工智能，会想到机器学习、深度学习、自然语言处理、数据统计、人工神经网络、遗传等各种术语。
但如果从广义上看，人工智能并不是一个孤立的领域，它是所有有助于超越人类能力的技术的集合。
让我们看一下人工智能中的一些研究领域：

**1. 机器学习（Machine Learn ML）**

机器学习是最受欢迎的研究领域之一，作为人工智能的核心子集，它赋予机器从数据中自主学习的能力，这与人类从经验中汲取知识的机制异曲同工。在实际应用中，机器学习系统会对海量数据集进行分析，挖掘数据背后的模式与规律，进而实现精准预测。例如，电商平台通过分析用户的历史购物数据、浏览行为，能够预测用户可能感兴趣的商品，实现个性化推荐，显著提升用户购物体验与平台销售额。

深度学习作为机器学习的前沿分支，其核心是模仿人类大脑神经网络结构与功能的人工神经网络（ANN）。传统的神经网络结构相对简单，处理复杂问题能力有限，而深度学习凭借多层神经网络架构，能够自动提取数据的深层次特征。以图像识别为例，在识别猫和狗的图片时，深度学习模型可以从像素层面开始，逐步提取线条、形状、纹理等特征，最终准确区分出不同的动物类别。如今，深度学习在语音识别、自动驾驶、医疗影像诊断等领域广泛应用，推动着这些行业的智能化变革。在自动驾驶中，深度学习模型能够实时分析车辆周围的图像、雷达数据，识别行人、车辆、交通标志等，为车辆的行驶决策提供依据，极大地提高了自动驾驶的安全性和可靠性。

**2. 逻辑学（Logic）**

逻辑是思维的规律与规则，是严谨推理的基石。在人工智能领域，逻辑同样扮演着至关重要的角色，其本质是为已知理由提供有效的证明体系。人类在面对问题时，会依据过往经验、知识和逻辑规则进行思考与决策，而人工智能系统要实现类人思考，也必须具备强大的逻辑推理能力。

以四色定理的证明为例，1976 年 7
月，美国的阿佩尔（K.Appel）等人借助三台大型计算机，耗费 1200 小时的 CPU
时间，并对中间结果进行 500
多处人为反复修改，最终成功证明了这一困扰数学界长达 124
年的难题。这一成就不仅是数学领域的重大突破，更彰显了逻辑推理与计算机算力结合在解决复杂问题上的巨大潜力。在人工智能系统中，逻辑推理被广泛应用于专家系统、知识图谱等领域。例如，在医疗诊断专家系统中，系统基于医学知识构建逻辑规则库，当输入患者的症状、检查结果等信息后，通过逻辑推理得出可能的疾病诊断结果，为医生提供辅助决策参考，提高诊断的准确性和效率。

**3. 自然语言处理(Natural Language Processing NLP)**

自然语言处理是人工智能的早期研究领域之一，致力于实现计算机对人类自然语言的理解与处理。其工作原理是计算机接收用户以自然语言形式输入的信息，如文本、语音等，然后通过内部预设的算法，对这些信息进行分词、词性标注、语义分析等系列操作，最终模拟人类对自然语言的理解，并输出用户期望的结果。

从实际应用场景来看，自然语言处理已深度融入我们的生活。智能语音助手，如苹果的
Siri、小米的小爱同学等，能够理解用户的语音指令，如查询天气、设置闹钟、播放音乐等，并迅速执行相应操作；机器翻译工具，如谷歌翻译、百度翻译，可实现不同语言之间的快速转换，打破语言交流障碍；智能客服系统在电商、银行等行业广泛应用，能够自动回答用户的常见问题，提高客户服务效率和质量。随着技术的不断进步，自然语言处理正朝着更加智能化、人性化的方向发展，例如情感分析技术能够识别文本中蕴含的情感倾向，为市场调研、舆情监测等提供有力支持；对话生成技术使得人机对话更加流畅自然，在虚拟聊天、智能写作等领域展现出巨大潜力。

**4. 模式识别（Pattern Recognition）**

随着计算机硬件性能的飞速提升以及应用领域的不断拓展，计算机需要更高效地感知声音、文字、图像、温度、震动等各类信息资料，模式识别技术应运而生并得到迅速发展。模式的原意是指可供模仿的完美标本，而在人工智能领域，模式识别旨在让计算机具备识别给定物体所模仿标本的能力，是对人类感知外界功能的高度模拟。

在图像识别方面，模式识别技术广泛应用于安防监控领域，通过对监控画面中的人脸、车牌等特征进行识别，实现人员追踪、车辆管理；在医疗领域，医学影像模式识别能够帮助医生识别
X 光、CT、MRI
等影像中的病变特征，辅助疾病诊断。在语音识别领域，语音助手、智能语音输入法等产品借助模式识别技术，将语音信号转化为文字信息，实现语音交互。此外，模式识别在手写文字识别、指纹识别、虹膜识别等生物特征识别领域也发挥着关键作用，为身份认证、安全防护等提供了可靠的技术支持。随着深度学习技术的引入，模式识别的准确率和效率得到进一步提升，能够处理更加复杂、多样化的数据。

**5. 知识发现（Knowledge Discovery in Data Base KDD）和数据挖掘（Data
Mining DM）**

知识发现（KDD）和数据挖掘（DM）是人工智能、机器学习与数据库技术深度融合的产物，在当今信息爆炸的时代背景下应运而生。随着互联网的蓬勃发展，全球数据量呈指数级增长，如何从海量数据中挖掘出有价值的信息（模式）和知识（规律），成为指导工作、生产、运营和销售策略，以及探索科学数据未知规律的关键问题，这也使得
KDD 和 DM 成为计算机领域的研究热点。

在商业领域，电商平台通过数据挖掘技术分析用户的购物行为、浏览历史、评价信息等数据，能够精准把握用户需求，优化商品推荐策略，提高用户购买转化率；金融机构利用
KDD 和 DM
技术对客户的信用数据、交易记录进行分析，评估客户信用风险，制定合理的信贷政策，降低坏账率。在科学研究中，天文学领域通过对海量天文观测数据的挖掘，发现新的星系、行星；生物学领域借助数据挖掘技术分析基因序列数据，探索基因与疾病之间的关系，为疾病治疗和药物研发提供新的思路。此外，KDD
和 DM
技术还在交通流量预测、气象数据分析、城市规划等多个领域发挥着重要作用，推动各行业的智能化发展。

**6. 智能决策支持系统（Intelligence Decision Supporting System IDSS）**

智能决策支持系统属于管理科学范畴，与 "知识 - 智能" 有着紧密联系。20 世纪
80
年代以来，专家系统在众多领域取得显著成功，将人工智能中的智能和知识处理技术引入决策支持系统，促使其升级为智能决策支持系统（IDSS）。

IDSS
结合了人工智能的知识推理、机器学习等技术与传统决策支持系统的数据处理、模型分析功能，能够为决策者提供更强大、更智能的支持。在企业管理中，IDSS
可以根据企业的财务数据、市场信息、生产运营状况等多维度数据，运用知识推理和模型分析，为企业制定战略规划、生产计划、市场营销策略等提供决策建议；在政府决策中，IDSS
可用于政策制定、城市规划、公共安全管理等领域，通过对大量社会数据的分析和挖掘，评估不同决策方案的潜在影响，帮助政府做出更科学、合理的决策。例如，在城市交通规划中，IDSS
可以综合考虑人口分布、交通流量、土地利用等因素，模拟不同交通建设方案的效果，为优化城市交通网络提供决策依据。

**7. 机器人学（Robot）**

机器人学是人工智能研究中备受关注的重要分支，主要聚焦于机器人装置程序的研究与开发。机器人和机器人学的发展极大地推动了人工智能思想的进步，其所衍生的技术能够模拟世界状态，精确描述从一种状态到另一种状态的转变过程，同时对动作序列规划及执行监督有着深入的理解与应用。

在工业生产领域，工业机器人广泛应用于汽车制造、电子装配、机械加工等行业，它们能够在高精度、高强度的工作环境下持续作业，提高生产效率和产品质量，降低人力成本和劳动强度。例如，汽车生产线上的焊接机器人，能够精准地完成汽车车身的焊接工作，确保焊接质量的一致性和稳定性。在服务领域，服务机器人如送餐机器人、清洁机器人、陪护机器人等逐渐走进人们的生活，为人们提供便捷、高效的服务。在医疗领域，手术机器人能够辅助医生进行精准的外科手术，减少手术创伤和并发症的发生；康复机器人则帮助患者进行康复训练，加速身体机能的恢复。此外，在太空探索、深海探测等极端环境下，机器人也发挥着不可替代的作用，它们能够代替人类完成危险、复杂的任务，拓展人类的探索边界。

**8. 分布式人工智能（Distributed AI DAI）和智能体（Agent）**

分布式人工智能（DAI）是分布式计算与人工智能相结合的创新成果，其核心研究目标是构建能够精确描述自然系统和社会系统的概念模型。在
DAI
中，智能并非孤立存在，而是依赖于多个智能体（Agent）之间的协作来实现，因此各
Agent
之间的合作与对话成为研究的关键问题，主要涵盖分布式问题求解（Distributed
Problem Solving，DPS）和多 Agent 系统（Multi-agent
System，MAS）两个领域。

在分布式问题求解中，多个智能体通过协同工作，将复杂问题分解为多个子问题并分别求解，最终整合结果得出完整解决方案。例如，在气象预报中，分布在不同地区的气象监测站（可视为智能体）收集当地的气象数据，然后通过分布式计算和智能体之间的信息交互，共同完成对大范围气象状况的预测，提高预报的准确性和及时性。多
Agent
系统则强调多个智能体在一定环境下自主运行、相互协作和竞争。在智能交通系统中，车辆、交通信号灯、道路传感器等都可看作智能体，它们通过信息共享和协同决策，优化交通流量，减少拥堵；在智能家居系统中，各种智能设备如灯光、空调、门锁等作为智能体，相互配合，为用户提供舒适、便捷、安全的居住环境。随着物联网、云计算等技术的发展，分布式人工智能和智能体技术将在更多领域得到应用和深化，推动智能化社会的发展。

### 6.3.2 人工智能的典型问题

**1. 搜索问题：找到解决问题的路径**

搜索问题是人工智能领域中最基础且重要的问题之一，其核心在于面对一个存在多种可能解决方案的问题，寻找从初始状态到目标状态的最优路径。每个解决方案都可看作是一系列有序步骤的组合，这些步骤构成了通向目标的路径。在实际情况中，不同解决方案可能包含重叠的路径子集，并且在效果和成本上存在差异。这里的
"更好" 取决于具体问题的目标，例如，在路径规划问题中，"更好"
可能意味着距离最短、时间最快；而 "更便宜"
通常指在计算资源消耗上更少，即可以利用性能相对较低、成本更廉价的计算机完成问题求解。

以地图上城市之间的路径规划为例，从城市 A 到城市 B
可能存在多条路线，这些路线的距离长短不一，所经路段的交通流量、路况条件也各不相同。传统的搜索算法如迪杰斯特拉算法（Dijkstra\'s
algorithm），通过计算图中节点之间的最短路径，能从所有可能的路线中找出距离最短的路径，但该算法在处理大规模地图数据时，计算复杂度较高，消耗的时间和内存资源较多。为了提高搜索效率，启发式搜索算法如
A \*
算法应运而生，它引入了启发函数，能够根据当前节点到目标节点的估计距离，优先搜索更有可能到达目标的路径，从而在保证找到最优解的前提下，大大减少了搜索空间，提高了搜索效率。在实际应用中，像百度地图、高德地图等导航软件，就是基于此类搜索算法，实时为用户规划出最优的出行路线，方便人们的日常出行。此外，在机器人路径规划、游戏角色寻路等领域，搜索问题也同样关键，不同场景下需要根据具体需求选择合适的搜索算法，以实现高效、准确的路径规划。

**2. 优化问题：找到一个好的解决方案**

优化问题在人工智能应用中广泛存在，其特点是面对大量的有效解决方案，要从中找到最优解并非易事。这类问题通常具有丰富的可能性，每个可能性在解决问题的能力上存在差异，而我们的目标就是在众多可能性中筛选出最符合需求的解决方案。

以汽车后备箱行李打包为例，后备箱的空间形状不规则，行李的大小、形状各异，存在无数种放置组合方式。如果能找到一种最优的打包方案，就能最大化利用后备箱空间，装载更多的行李。在实际求解优化问题时，往往会涉及到局部最优和全局最优的概念。局部最优解是指在搜索空间的某个局部区域内的最优解，而全局最优解则是整个搜索空间中的最优解。一般情况下，搜索空间中会存在多个局部最优解，而全局最优解只有一个。例如，在寻找最佳餐厅的问题中，我们可能在某个城市或区域内找到了评价最高、最受欢迎的餐厅，这就是局部最优解，但从全国或全球范围来看，可能存在更优质的餐厅，即全局最优解。

为了解决优化问题，人们开发了多种算法。遗传算法模拟生物进化过程中的自然选择和遗传机制，通过选择、交叉、变异等操作，在解空间中不断搜索更优解；粒子群优化算法则模拟鸟群觅食行为，通过粒子在解空间中的运动和信息共享，逐步逼近最优解。这些算法在工程设计、资源分配、生产调度等领域都有广泛应用。例如，在工程设计中，通过优化算法可以在满足各种性能指标和约束条件的前提下，找到最合理的设计参数，降低生产成本，提高产品性能；在资源分配中，能够实现资源的最优配置，提高资源利用效率。

**3. 预测和分类问题：从数据模式中学习**

预测和分类问题是人工智能从数据中学习和提取价值的重要体现。预测问题旨在基于已有的数据，挖掘其中隐藏的模式和规律，从而对未来或未知的情况进行预测。例如，在汽车油耗预测中，我们收集了不同车辆的发动机尺寸、车辆重量、行驶路况等数据，通过分析这些数据与油耗之间的关系，建立数学模型。如果数据显示发动机尺寸与油耗之间存在显著的相关性，那么就可以利用这个模型，根据新车型的发动机尺寸等参数，预测其油耗情况。常见的预测算法包括线性回归、时间序列分析等。线性回归通过拟合数据点，找到变量之间的线性关系，从而进行预测；时间序列分析则关注数据随时间的变化趋势，用于预测未来的数值。

分类问题与预测问题类似，但侧重点不同。分类问题是根据事物的特征，将其划分到不同的类别中。例如，根据车辆的尺寸、发动机尺寸、座椅数量等特征，判断车辆是摩托车、轿车还是
SUV。在解决分类问题时，需要从数据中找到能够有效区分不同类别的模式。决策树算法是一种常用的分类算法，它通过构建树形结构，根据数据特征的不同取值逐步划分数据，最终将数据分到不同的类别中；支持向量机则通过寻找最优超平面，将不同类别的数据在特征空间中进行分离。分类问题在图像识别、文本分类、疾病诊断等领域有着广泛的应用。在图像识别中，通过对图像的颜色、纹理、形状等特征进行分析，将图像分类为不同的物体类别；在文本分类中，根据文本的词汇、语法、语义等特征，将文本分类为新闻、小说、评论等不同类型。

**4. 聚类问题：识别数据中的模式**

聚类问题致力于从数据中挖掘潜在的趋势和关系，通过对数据不同特征的分析，将数据样本进行分组，使得同一组内的数据具有较高的相似性，而不同组之间的数据具有较大的差异性。与预测和分类问题不同，聚类问题不需要预先设定明确的目标类别，而是自动发现数据中的内在结构。

以餐馆数据聚类为例，我们收集了餐馆的成本、位置、菜品类型、顾客评价等数据。通过聚类分析，可能会发现一些有趣的模式：年轻人倾向于经常光顾食物价格便宜、位于商业中心且菜品口味偏重的餐馆；家庭顾客则更偏好环境舒适、菜品丰富且适合儿童的餐馆。在实际应用中，聚类算法有很多种，K -
均值聚类算法是最常用的一种。它首先随机选择 K
个聚类中心，然后根据数据点与聚类中心的距离，将数据点分配到最近的聚类中，接着更新聚类中心，重复这个过程，直到聚类结果稳定。层次聚类算法则通过构建数据的层次结构，从单个数据点开始，逐步合并相似的数据点或聚类，直到达到预定的聚类数量或条件。

聚类问题在市场细分、客户关系管理、图像分割等领域具有重要的应用价值。在市场细分中，通过对消费者的购买行为、偏好等数据进行聚类，企业可以将市场划分为不同的细分市场，针对不同细分市场的特点制定个性化的营销策略；在图像分割中，将图像中的像素点进行聚类，划分出不同的区域，有助于提取图像中的目标物体，为图像分析和理解提供基础。

**5. 确定性模型：每次计算的结果都相同**

确定性模型是指在给定相同输入的情况下，无论何时进行计算，都会返回相同输出的模型。这类模型基于明确的规则和逻辑关系，不存在随机因素的影响。例如，在计算圆的面积时，只要给定圆的半径，根据圆的面积公式S
= \\pi
r\^2，每次计算得到的结果都是确定的。在时间与昼夜关系的简单模型中，给定一个城市的中午时间，基于常规的认知和时间规律，我们可以确定该城市处于白天；如果给定的是午夜时间，就可以确定该城市处于黑暗状态。

然而，这种简单的确定性模型往往是基于理想化的假设，在现实世界中，实际情况可能更为复杂。以昼夜模型为例，在地球两极附近地区，由于地球的公转和地轴倾斜，存在极昼和极夜现象，即一天中可能全是白天或全是黑夜，这就说明简单的确定性模型没有考虑到特殊地理区域的不寻常日光持续时间。在人工智能领域，确定性模型常用于一些规则明确、条件固定的场景，如简单的逻辑推理、数据计算等。例如，在一些基于规则的专家系统中，根据输入的条件和预先设定的规则，确定性地得出结论；在数据处理中，对数据进行确定性的转换和计算，如数据的标准化、归一化处理等。

**6. 随机/概率模型：每次计算的结果都可能不同**

概率模型与确定性模型不同，它在给定输入时，会根据一定的概率从一组可能的结果中返回某个结果，模型中包含受控的随机性元素。这种随机性使得概率模型能够更好地描述现实世界中存在的不确定性和多样性。例如，在天气预测中，即使给定的时间是中午，由于天气系统的复杂性和不确定性，天气可能是晴天、多云或下雨，每种天气状况都有一定的发生概率。

在概率模型中，常用概率分布来描述不同结果出现的可能性。常见的概率分布有正态分布、泊松分布、二项分布等。以抛硬币为例，每次抛硬币出现正面或反面的结果是随机的，且正面和反面出现的概率均为
0.5，这可以用二项分布来描述。在人工智能中，概率模型广泛应用于自然语言处理、语音识别、机器学习等领域。在自然语言处理中，语言的表达具有多样性和不确定性，概率模型可以用于计算词语出现的概率、句子的生成概率等，从而实现语言的理解和生成；在机器学习中，概率模型可以用于估计数据的概率分布，进行分类和预测，如朴素贝叶斯分类器就是基于概率模型的一种分类算法，它通过计算每个类别在给定特征下的概率，将数据分类到概率最大的类别中。概率模型为人工智能处理不确定性问题提供了有效的方法，使模型能够更好地适应复杂多变的现实环境。

## 6.4人工智能与社会、伦理问题

人工智能的快速发展在给人类社会进步和经济发展带来巨大利益的同时，也带来严峻挑战。就像任何新技术一样，人工智能的发展也引起或即将出现许多问题，并使一些人感到担心或懊恼。社会上一些人担心人工智能技术会抢夺他们的饭碗而使他们失业，担忧智能机器人的智慧超过人类而威胁人类安全等。这些问题涉及劳务就业、社会结构变化、思维方式与观念的变化、法律、伦理道德、心理威胁和技术失控危险等。这些都是值得高度关注的影响社会安定和谐的社会问题。

### 6.4.1就业与社会结构问题

由于人工智能能够代替人类进行各种脑力劳动和体力劳动，将有一部分人员可能把自己的工作让位给机器人，造成他们的下岗和再就业，甚至造成失业。2013年英国牛津大学的一项研究报告称：未来有700多种职业都有被智能机器替代的可能性。越是可以自动化、计算机化的任务，就越有可能交给智能机器完成，其中以行政、销售、服务业首当其冲。广泛采用人工智能技术和智能机器替代员工从事各种劳动，这是由智能机器的优点决定的。这些优点包括：提高生产效率，降低运营成本；提高产品质量的稳定性与一致性；缩短产品改型换代的准备周期，减少相应设备投资；降低对一线操作人员的技能要求，改善劳动环境；节约人工工资和相关费用，提高生产安全；更加灵活，大大提升批量化生产效率，实现小批量产品的快速响应；更加信息化，使得生产制造过程中的材料、半成品、成品、各种工艺参数等信息更加容易采集，方便质量控制、质量分析、产品制造过程追溯，同时产品的成本统计和控制也更加容易。智能机器特别能够代替人们从事危险和不愿意从事的工作。采用人工智能技术还与直接经济效益有密切关系。以机器人为例，根据机器人平均价格指数和劳务报酬指数曲线，机器人的平均价格一直下降（如10年降价一半），而劳务报酬却一直上升（如10年提高一倍）；也就是说，10年间劳务报酬指数与机器人价格指数的比值提高了4倍。因此，企业愿意采用机器人换人，世界各国必然更多地应用各种机器人（含工业机器人和服务机器人等）以替代人工劳动，这已成为21世纪的一个必然趋势。这是一个值得社会学家、经济学家、人口学家以及政府决策官员们高度关注和深入研究的紧迫社会问题。

人们一方面希望人工智能和智能机器能够代替人类从事各种劳动，另一方面又担心它们的发展会引起新的社会问题。实际上，数十年来，社会结构正在发生一种静悄悄的变化。过去人们直接与机器打交道，而现在要通过智能机器与传统机器打交道。也就是说，人‒机器的社会结构，终将为人‒智能机器‒机器的社会结构所取代。智能机器人就是一种智能机器。从发展的角度看，从医院里看病的"医生"、护理病人的"护士"，旅馆、饭店和商店的"服务员"，办公室的"秘书"，指挥交通的"警察"，到家庭的"勤杂工"和"保姆"等等，将均由机器人来担任。因此，人们将不得不学会与智能机器相处，并适应这种变化了的社会结构。由于与机器人打交道毕竟不同于与人打交道，所以人们必须改变自己的传统观念和思维方式，逐步适应与智能机器的共处。

### 6.4.2心里上的威胁与技术失控

智商（IQ）即智力商数，为个人智力测验成绩和同年龄人被试成绩相比的指数，是衡量个人智力高低的一个标准。设定人的平均IQ值为100，据预测，随着计算机智能程度的不断提高，20年后人工智能的IQ将达到10,000。另一个数据，预测未来20年智能机器人（含工业机器人、服务机器人、智能驾驶汽车等）的数量会达到100亿以上，将超过人口总量。从这两个数据可以看到，就某些重要指标而言，20年后智能机器人将从数量到质量全面赶超人类。

未来的人工智能系统和智能机器将发展到非常"聪明"的程度，将能够自动识别并解决人类无法应对的体力和智力问题。智能机器将比人类至今已经制造出来的任何机器更有智慧和更加有用，它们不仅是工具，更是具有创造性的机器

人工智能使一部分社会成员感到心理上的威胁，或叫做精神威胁。人们一般认为，只有人类才具有感知精神，而且以此与机器相别。如果有一天，这些人开始相信机器也能够思维和创作，那么他们可能会感到失望，甚至受到威胁。他们担心：有朝一日，智能机器的人工智能会超过人类的自然智能，使人类沦为智能机器和智能系统的奴隶。对于人的观念（更具体地指人的精神）和机器的观念（更具体地指人工智能）之间的关系问题，哲学家、神学家和其他人们之间一直存在着争论。按照人工智能的观点，人类有可能用机器来规划自己的未来，甚至可以把这个规划问题想象为一类状态空间内的搜索求解过程。当社会上一部分人欢迎这种新观念时，另一部分人则发现这些新观念是惹人烦恼的和无法接受的，尤其是当这些观念与他们钟爱的信仰和观念背道而驰时。

任何新技术的最大危险莫过于人类对它失去了控制，或者是它落入那些企图利用新技术反对人类的人手中。我国引以自豪的火药发明被某些外国人用于制造炸弹；化学科学的成果被人用于制造化学武器；生物学的最新成就可能被用于制造生物武器；核物理研究的重大突破导致原子弹和氢弹的威胁。难怪现在有人担心机器人有一天会反宾为主，奴役它们的创造者------人类，担心人工智能的其它智能制品威胁人类的安全。正是由于认识到这种担心，著名的美国科幻作家阿西莫夫提出了"机器人三守则"：（1）机器人必须不危害人类，也不允许它眼看人类受害而袖手旁观。（2）机器人必须绝对服从人类，除非这种服从有害于人类。（3）机器人必须保护自身不受伤害，除非为了保护人类或者是人类命令它做出牺牲。我们认为，如果把这个"机器人三守则"推广到整个智能机器，成为"智能机器三守则"，那么，人类社会就会更容易接受智能机器和人工智能。可惜至今未能创造出一种能够鉴别实际上是否遵循"三守则"的人工智能软件。一个人工智能系统可能与其它一些机构结合起来，力图遵循这些规则，但尚无足够的智能来确保长期地遵守这些规则。人工智能技术是一种信息技术，能够极快地传递。因此，存在某些比爆炸技术更大的潜在危险，即人工智能技术可能落入不负责的人手中，被他们用于进行反人类和危害社会的犯罪（有的人称之为"智能犯罪"）。对此，我们必须保持高度警惕。

### 6.4.3 法律与伦理问题

人类将对具有人工意识的机器人给予前所未有的关注。机器人的发展与应用带来了许多法律问题，例如，汽车机器人发生事故，造成伤害应该由谁负责?又如，自主机器人士兵在战场上开枪打死人类是否违反国际公约？再如，机器人参加运动会赛跑可能会要求得到奖赏，并由此获得收入。随着思想复杂性的提高，人形机器人可能开始表达对于生活现象和问题的观点，甚至提出政治主张，要求拥有言论自由和游行示威权利。这些现象可能给人类带来挑战、不安，甚至危险。有些学者认为：机器人与人类反目成仇的可能性远高于我们的预想。机器人可能与人类对抗，它们不具备人类那种良知意识、是非观念和人生观念。用于对实施暴力、人身伤害甚至杀人行为的人类法律，是非也适用于机器人？机器人法律问题已经提上议事日程。阿西莫夫提出的"机器人三法律"或"机器人三法则"的目的是创造一个基本框架，以便让机器人拥有一定程度的"自我约束"。虽然这样复杂的机器人目前还没有制造出来，但机器人法律却已被广泛应用在在科幻、电影以及机器人研发领域中的"机器人学"和"人工智能"领域上。不过，阿西莫夫三法则有着一定的历史局限性和概念歧义性，三条法则之间也存在逻辑矛盾。假如两个人斗殴，机器人应当选择帮谁呢？加入任一方都违反第一法则前半部分，不加入却又违反第一法则后半部分。假如主人要机器人帮他去抢银行，"抢银行"也是主人的一种命令，也不违反第一法则，那么机器人一定是去抢银行。这种违法的事情算不算违背第一法则？假如机器人在执行"保护人质"命令的过程中，未能保护自己而被绑架者"击毙"，算不算机器人违背了第三法则？由此可见，机器人三法则自身就存在着严重的逻辑矛盾。在全面迎来无人驾驶的时代，主要根据驾驶人员过错划分责任的道路交通制度体系将被彻底颠覆。一旦发生交通事故，现有的驾驶员责任不复存在，汽车公司可能将成为保险公司最大客户，交通法规或将改写。在人工智能时代，大量的作品可能由智能机器创作，如记者机器人撰写新闻稿。这一切或将颠覆现有的与知识产权相关的法律制度。"高级案件管理"系统，也被称为"机器人法官"。该系统能够通过对既有数据的判例与分析，自动生成最优的判决结果。与法官一样面临职业危机的还有律师、教师、艺术家等行业。在人工智能时代，法律也将重塑对他们的职业要求。

人们所担忧的应用服务机器人引发的风险与伦理问题，主要涉及两个方面：一是小孩和老人的看护，另一是军用自主机器人武器的发展。孩子与机器人短期接触可以提供愉悦的感受，还可以激发他们的兴趣和好奇心。但是，机器人还不能作为孩子的看护者，因为孩子始终需要大人来照顾。过长时间与机器人相处，可能会造成孩子成长过程中不同程度的社会孤立。如果将孩子完全交给一个机器人保姆，让机器人保姆照料孩子的安全，那么可能使孩子缺失社交功能。人口老年化程度加深促进了助残机器人的发展。例如，喂食机器人、洗澡机器人、提醒用药的监控机器人等。老年人长期完全置于机器人照顾之下也是存在风险的。老年人需要与人接触，需要看护者为他们提供日常护理工作。即使它们可以减轻老年人的亲友的内疚感，却无益于解决老年人的孤独问题。人们正在逐渐被现实世界中人与机器人之间的关系所误导。军用机器人的应用也产生另一些道德问题。例如，美国在伊拉克和阿富汗配置了5000多个遥控机器人，用于侦察与排雷，也有少数用于作战的重型武装机器人。在作战行动中，由地面机器人充当先锋，当隐藏着的敌人攻击时，无人驾驶侦察机发现敌军位置，通知巡航中的战斗机，然后，战斗机发射导弹，命中目标。这一切行动都通过网络由战场上的美军操控。地面武装机器人和自主/半自主武装无人机造成许多无辜群众（包括儿童、妇女）的伤亡。如在叙利亚、伊拉克、阿富汗经常出现的那样。通常，武器都是在人的控制下选择合适的时机发动致命打击。然而，军用机器人可在无人控制的条件下自动锁定目标并且推毁它们。这些伦理问题的出现是因为目前还没有能够在近距离遭遇战中区别战斗人员和无辜人员的运算系统。计算机程序需要一个非常清晰的非战斗人员的定义，但目前还没有这种定义。机器人被设计用于实施包括军事活动或违法行为在内的对抗人类的行动。自主机器人士兵将大大增加引发地区性冲突和战争的可能性。人形机器人的发展引发对伦理道德的关注。如果不谨慎对待，那么机器人可能使我们受到伤害：造成伤害事故、实施故意伤害或犯罪行为、可能直接接触我们的个人隐私信息，并把它泄露给公众。人类如何才能让智能机器具有同情心、责任心、同理心、负罪感、羞耻感和是非判断能力，如何通过编程实现机器人道德?否则，人形机器人就可能具有反社会和毁灭人类的倾向。我们希望制作出和善的智能机器，而不希望它们完全仿效人类，不希望它们仿效残忍、自私、自以为是、为了一己私利而不择手段等不良品性。

## 6.5人工智能大模型

大模型通常是指那些通过大规模数据和强大计算能力训练出来的具有海量参数的模型。这些模型的参数数量可达数亿甚至上百亿、千亿级别，远远超过传统的机器学习模型。其核心在于利用深度学习技术，构建复杂的神经网络架构，以对海量数据进行深度挖掘和学习，从而具备强大的表示能力和泛化能力。

大模型能够自动从数据中提取特征和模式，不仅可以精准地捕捉到数据中的细微差别和潜在规律，还能够在不同的领域和任务中展现出通用性，无需针对每个具体任务进行大规模的重新训练，只需通过微调调整，甚至直接应用，即可完成如自然语言处理、图像识别、语音识别、机器翻译等多样化的复杂任务。

大模型为人工智能的广泛应用提供了坚实的技术支撑，极大地推动了人工智能向更高水平发展，成为当今人工智能领域的关键技术之一，引领着人工智能技术的发展潮流。

大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了智能的涌现，展现出类似人类的智能。

### 6.5.1 大模型的发展历史

大模型的发展历程可以追溯到上世纪中叶，经历了多个重要阶段，逐步演进并走向成熟。**1.
萌芽期（1950 - 2005）**

这一时期是以CNN（卷积神经网络）为代表的传统神经网络模型阶段。1980年，卷积神经网络的雏形诞生。1998年，现代卷积神经网络的基本结构LeNet -
5出现，机器学习方法从早期的浅层机器学习模型转变为基于深度学习的模型，为自然语言生成、计算机视觉等领域的深入研究奠定了基础，对后续深度学习框架的迭代及大模型发展具有开创性意义，标志着人工智能开始向大规模模型探索的方向迈出了第一步。**2.
沉淀期（2006 - 2019）**

此阶段是以Transformer为代表的全新神经网络模型的发展时期。2013年，自然语言处理模型Word2Vec诞生，首次提出将单词转换为向量的"词向量模型"，便于计算机更好地理解和处理文本数据。2014年，GAN（对抗式生成网络）诞生，被誉为21世纪最强大算法模型之一，标志着深度学习进入了生成模型研究的新阶段。2017年，Google提出基于自注意力机制的神经网络结构，Transformer架构，为大模型预训练算法架构奠定了基础，极大地提升了模型处理长序列数据的能力，使得模型能够更好地捕捉数据中的复杂依赖关系。2018年，OpenAI和Google分别发布了GPT -
1与BERT大模型，预训练大模型开始成为自然语言处理领域的主流，推动了大模型技术在自然语言处理等领域的广泛应用和深入研究，众多研究团队开始基于这些基础模型进行拓展和优化，大模型技术在这一时期得到了不断沉淀和积累，性能逐步提升，应用范围也逐渐扩大。**3.
爆发期（2020 - 至今）**

以GPT为代表的预训练大模型阶段开启了大模型的爆发式增长。2020年，OpenAI公司推出了GPT -
3，其模型参数规模达到了1750亿，成为当时最大的语言模型，并且在零样本学习任务上实现了巨大性能提升，展示了大模型强大的语言生成和理解能力，引发了全球范围内对大模型的广泛关注和研究热潮。随后，一系列基于人类反馈的强化学习（RHLF）、代码预训练、指令微调等策略被应用于进一步提高大模型的推理能力和任务泛化能力，使得大模型能够更好地理解和执行人类的指令，在各种复杂任务中表现出更加优异的性能。2022年11月，搭载了GPT3.5的ChatGPT问世，凭借其逼真的自然语言交互与多场景内容生成能力，迅速在互联网上引起轰动，吸引了大量用户的使用和关注，进一步推动了大模型在自然语言处理领域的应用普及和商业价值的挖掘，也促使更多的企业和研究机构加大在大模型领域的投入和研发力度。2023年3月，GPT -
4发布，具备了多模态理解与多类型内容生成能力，不仅能够处理文本数据，还能够理解和生成图像等其他模态的数据，进一步拓展了大模型的应用场景和能力边界，标志着大模型技术进入了一个新的发展阶段，朝着更加通用、智能和强大的方向发展。

在国内，大模型的研究也呈现出蓬勃发展的态势。百度的**文心大模型**涵盖了语言、视觉、跨模态等多个领域，通过不断优化模型架构和训练算法，提升了模型的性能和应用能力，在智能搜索、智能写作、智能对话等场景中得到了广泛应用。华为的**盘古大模型**在自然语言处理、计算机视觉、多模态等领域也取得了显著进展，其在大规模数据处理和模型训练优化方面的技术优势，使得盘古大模型能够在复杂的工业场景中发挥重要作用。此外，字节跳动、腾讯、阿里等科技巨头也纷纷加大在大模型领域的研发投入，推出了各自的大模型产品，并在实际应用中不断探索和创新，推动了国内大模型技术的整体发展水平不断提升。

### 6.5.2 大模型的关键技术和特点

**1. 深度学习是基础**

深度学习作为大模型的核心技术基石，其原理在于构建具有多个层次的神经网络，旨在模拟人类大脑神经元的信息处理方式，以实现对复杂数据的深度理解和特征提取。在大模型中，深度学习的多层神经网络结构发挥着关键作用，通过堆叠多个神经元层，使得模型能够自动从海量数据中学习到高度抽象和复杂的特征表示，从而具备强大的模式识别能力。

在图像领域，大模型首先通过卷积神经网络（CNN）的卷积层对图像数据进行局部特征提取，这些特征可以是图像的边缘、纹理、颜色等信息，随着网络层数的增加，后续的全连接层能够将这些局部特征进一步整合为更高级、更抽象的语义特征，从而实现对图像中物体的准确识别和分类。

在自然语言处理领域，循环神经网络（RNN）及其变体如长短期记忆网络（LSTM）和门控循环单元（GRU），能够有效处理文本序列中的上下文信息，捕捉词语之间的语义关联和语法结构，使得模型能够理解文本的含义并生成连贯、合理的回复，为语言模型的发展奠定了基础。

**2. 核心组件与算法**

神经网络架构在大模型中占据着核心地位，其设计的合理性和先进性直接决定了模型的性能表现。主流的神经网络架构如Transformer架构，通过引入自注意力机制，彻底改变了模型对序列数据的处理方式。在自然语言处理任务中，自注意力机制使得模型能够在处理文本时，动态地关注输入序列中的不同部分，根据上下文信息为每个单词分配不同的权重，从而更加精准地捕捉单词之间的语义关系和语法结构，有效解决了传统神经网络在处理长序列数据时面临的信息丢失和梯度消失等问题，极大地提升了模型对自然语言的理解和生成能力。

参数优化算法是大模型训练过程中的关键环节，其作用在于通过不断调整模型的参数，使得模型的预测结果与真实值之间的误差最小化，从而提高模型的准确性和泛化能力。常见的参数优化算法包括随机梯度下降（SGD）及其变种Adagrad、Adadelta、Adam等。这些算法在训练过程中，根据损失函数对模型参数的梯度信息，以一定的学习率逐步更新参数，使得模型能够朝着最优解的方向不断逼近。例如，Adam优化算法结合了动量法和自适应学习率的思想，在训练过程中能够根据参数的梯度历史信息，自适应地调整每个参数的学习率，使得模型在训练过程中能够更快地收敛到较好的结果，同时减少了训练过程中的震荡和过拟合风险，提高了模型的训练效率和稳定性。

注意力机制作为大模型中的重要创新点，除了在Transformer架构中的自注意力机制外，还包括多头注意力机制等扩展形式。多头注意力机制通过并行地使用多个注意力头，每个注意力头关注输入序列的不同方面，然后将各个注意力头的结果进行拼接和线性变换，进一步增强了模型对复杂语义信息的捕捉能力。在机器翻译任务中，多头注意力机制能够同时关注源语言句子中的词汇、语法、语义等多个层面的信息，以及目标语言句子中的上下文信息，从而更加准确地生成高质量的翻译结果，提升了模型在跨语言交流中的性能表现，为自然语言处理领域的发展带来了新的突破和机遇。

**3. 大规模数据采集与预处理**

大规模数据采集是大模型训练的基础，其来源广泛，涵盖了互联网文本、社交媒体数据、学术文献、书籍、新闻报道以及各类专业数据库等多个领域，为模型提供了丰富多样的信息，使其能够学习到不同领域、不同语境下的语言表达和知识体系，从而具备广泛的适用性和强大的泛化能力。

以社交媒体数据采集为例，推特（Twitter）、微博等平台每天都会产生海量的用户生成内容，包括文本、图片、视频等多种形式。通过网络爬虫技术，可以按照特定的规则和关键词，从这些平台上抓取大量与特定主题相关的数据，如对于自然语言处理中的情感分析任务，可以采集用户在社交媒体上对某一产品、事件或话题的评论和讨论数据。然而，采集到的数据往往存在着噪声大、质量参差不齐、格式不统一等问题，因此需要进行有效的预处理，以提高数据的质量和可用性，为后续的模型训练提供坚实的基础。

数据清洗是预处理的重要环节之一，其目的在于去除数据中的无效、重复、错误或不完整的部分。在文本数据中，可能存在大量的HTML标签、特殊字符、乱码、拼写错误以及无意义的停用词等，这些都会干扰模型的学习和理解。通过使用正则表达式、自然语言处理工具和字典匹配等方法，可以有效地去除这些噪声信息，将文本数据转换为干净、规范的格式。例如，对于HTML标签，可以使用正则表达式匹配并删除；对于拼写错误的单词，可以借助拼写检查工具或基于词典的方法进行纠正；对于停用词，如常见的"的""是""在"等，由于它们在语义表达上的贡献相对较小，可以将其从文本中过滤掉，从而减少数据的冗余，提高数据的质量和处理效率。

数据标注是为数据赋予特定的标签或注释，以便模型能够学习到数据中的语义信息和任务相关的特征。在图像识别任务中，需要对采集到的图像数据进行标注，标注内容可以包括图像中物体的类别、位置、大小等信息；在自然语言处理任务中，数据标注可以是对文本的情感倾向（如正面、负面、中性）、文本的主题类别（如体育、科技、娱乐等）、词性标注、命名实体识别（如人名、地名、组织机构名等）等。标注的准确性和一致性对于模型的训练效果至关重要，通常需要专业的标注人员或使用自动化标注工具与人工审核相结合的方式来确保标注质量。

数据增强是通过对原始数据进行一系列变换操作，生成更多具有多样性的数据样本，从而扩充数据集的规模，增强模型的泛化能力，避免过拟合现象的发生。在图像数据中，常见的数据增强方法包括随机裁剪、翻转、旋转、缩放、颜色变换等，这些操作可以在不改变图像语义信息的前提下，增加图像的多样性，使模型能够学习到图像在不同视角、光照条件和姿态下的特征表示。例如，对于一张汽车的图片，可以通过随机裁剪出汽车的不同部位，或者对图片进行水平翻转、垂直翻转等操作，生成多个不同的图像样本，让模型学习到汽车的各种特征，提高其对不同形态汽车的识别能力。在文本数据中，数据增强的方法包括同义词替换、随机插入、随机删除、语序变换等，例如将文本中的"美丽"替换为"漂亮""秀丽"等同义词，或者在文本中随机插入一些修饰词，改变句子的语序等，从而生成与原始文本语义相近但表述略有不同的新文本，丰富模型的训练数据，提升其对语言的理解和生成能力。

通过这些数据采集与预处理步骤，可以为大模型提供高质量、多样化、丰富的训练数据，为模型的性能提升奠定坚实的基础。

**4. 特点**

- **巨大的规模** 大模型包含数十亿个参数，模型大小可以达到数百 GB
  甚至更大。巨大的模型规模使大模型具有强大的表达能力和学习能力。

- **涌现能力**
  或称创发、突现、呈展、演生，是一种现象。为许多小实体相互作用后产生了大实体，而这个大实体展现了组成它的小实体所不具有的特性。引申到模型层面，涌现能力指的是当模型的训练数据突破一定规模，模型突然涌现出之前小模型所没有的、意料之外的、能够综合分析和解决更深层次问题的复杂能力和特性，展现出类似人类的思维和智能。涌现能力也是大模型最显著的特点之一。

- **更好的性能和泛化能力**
  大模型通常具有更强大的学习能力和泛化能力，能够在各种任务上表现出色，包括自然语言处理、图像识别、语音识别等。

- **多任务学习** 大模型通常会一起学习多种不同的 NLP
  任务,如机器翻译、文本摘要、问答系统等。这可以使模型学习到更广泛和泛化的语言理解能力。

- **大数据训练** 大模型需要海量的数据来训练,通常在 TB 以上甚至 PB
  级别的数据集。只有大量的数据才能发挥大模型的参数规模优势。

- **强大的计算资源** 训练大模型通常需要数百甚至上千个
  GPU,以及大量的时间,通常在几周到几个月。

- **迁移学习和预训练**
  大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能。

- **自监督学习**
  大模型可以通过自监督学习在大规模未标记数据上进行训练，从而减少对标记数据的依赖，提高模型的效能。

- **领域知识融合**
  大模型可以从多个领域的数据中学习知识，并在不同领域中进行应用，促进跨领域的创新。

- **自动化和效率**
  大模型可以自动化许多复杂的任务，提高工作效率，如自动编程、自动翻译、自动摘要等。

### 6.5.3 自然语言处理大模型

自然语言处理（Natural Language
Processing，NLP）是人工神经网络最复杂的问题。除了前面的所有技术之外，我们还需要语言模型（Language
Model，LM）才能完成对自然语言的处理。我们生成了一些语言模型，而这当中最有名的一个语言模型当属GPT（Generative
Pre-Trained Transformer）。

**1. OpenAI GPT-4 系列**

Radford, A. et al 在2018年的论文\"Language Models are Unsupervised
Multitask
Learners\"介绍了生成的预训练模型GPT-2。以令人印象深刻的能力在各种语境中产生高质量的文本。目前，GPT-4系列发布。该模型基于Transformer架构，核心是自注意力机制。这种机制能够让模型在处理文本时，动态地关注输入序列中的不同部分，从而更好地捕捉文本中的长距离依赖关系和上下文信息，例如在处理长篇小说、复杂的技术文档等时，能够理解各部分之间的语义关联。

同时，在模型中混合了专家模型。GPT-4是由多个混合专家模型组成的集成系统，每个专家模型都有庞大的参数数量，共有16个专家模型。在推理过程中，每次前向传递会选择其中2个专家模型进行路由，这种结构有助于在保持性能的同时降低计算成本和内存消耗，并使模型能够处理更复杂的任务和数据分布。

GPT-4拥有约1.8万亿个参数，是GPT-3的10倍以上，这使其能够学习到更丰富的语言知识和语义表示，从而生成更准确、更复杂、更符合逻辑的文本。

GPT-4具备多模态能力，在纯文本的预训练基础上，增加了独立的视觉编码器，并通过交叉注意力机制实现文本与图像的融合，使其能够处理图像输入并生成相关的文本描述。例如可以根据输入的图片生成图片的文字说明以及回答与图片内容相关的问题等。

**2. Google DeepMind Gemini**

Gemini
的基础架构是Transformer架构，并结合了混合专家模型。和ChatGPT一样，每个专家模型在训练数据的不同子集上进行训练，然后通过门控网络根据输入来分配权重，将这些专家模型的预测结果进行组合，从而得到最终的预测，这种结构让模型能够处理非常复杂的任务并泛化到各种领域.

Gemini 2.0
具有原生多模态架构，支持文本、图像、视频、音频和代码的统一理解与生成。输入端多模态能够直接摄取不同模态的数据作为交错序列，在处理多模态信息时，不是简单的模态拼接，而是从一开始就在不同模态上进行预训练，并利用额外的多模态数据微调，使其在初始输入阶段就能对各种内容进行快速理解和推理。输出端多模态首次实现原生图像生成和定向文本转语音（TTS）能力，还新增了
Multimodal Live API，支持音视频实时流处理。

Gemini 有不同的版本，如 Gemini Ultra、Gemini Pro 和 Gemini Nano
等，以适应不同的应用场景和设备。Gemini Ultra
是最强大的版本，具备卓越的性能，可在各种高度复杂的任务中提供先进性能；Gemini
Pro
在成本和延迟方面进行了性能优化，能在广泛任务范围内提供较优性能，并表现出强大的推理性能和广泛的多模态能力；Gemini
Nano
则是针对数码设备优化的最高效模型，可在移动设备上执行本地任务，如聊天应用中的建议回复或文本总结等，通过从更大的
Gemini 模型中提取精髓进行训练，并经过量化以实现最佳性能。

该模型与谷歌基础设施的深度融合，借助谷歌的张量处理单元（TPU）进行训练和推理。TPU是谷歌专门为机器学习发明并针对TensorFlow进行优化的定制设计硅芯片，能够更快地训练和运行人工智能模型，为
Gemini 的大规模训练和高效服务提供了强大的计算支持。从最初的 TPUv1
发展到如今的 TPUv5p 等版本，不断提升模型的训练速度和性能。

**3. 文心一言**

文心一言的基础架构也是 Transformer
架构，并在此基础上进行了知识增强。知识增强则是通过引入大规模的知识图谱等知识数据，让模型能够更好地理解和生成与知识相关的文本内容，使生成的文本更具专业性和准确性。文心大模型形成了基础-任务-行业三级大模型体系。基础大模型包括自然语言处理、视觉、跨模态等基础模型，为各种任务和行业应用提供基础支撑；任务大模型则针对对话、跨语言、搜索、信息抽取等具体任务进行优化和定制；行业大模型则进一步结合特定行业的数据和需求，为不同行业提供更具针对性的解决方案。

支持多模态输入与输出，能够处理文本、语音、图像等多种模态的信息，并实现跨模态的理解和生成。用户可以输入文本描述，要求生成相应的图像，或者输入图像，要求文心一言生成与之相关的文本描述等。

文心一言 3.5
版本新增了插件机制，如官方插件百度搜索和chatfile等。百度搜索插件可以让模型实时获取最新的信息，生成更准确、更具时效性的回答。chatfile插件则可以帮助用户快速生成长文本摘要，提高信息处理效率，进一步拓展了大模型的能力边界。

**4. Meta AI LLaMA 3 **

语言大模型的基础架构基本上大同小异。但是LLaMA采用仅解码的 Transformer
架构，这种架构在处理语言生成任务时具有高效性和灵活性。它能够根据输入的文本序列，逐步生成后续的文本，适用于各种自然语言处理任务，如文本生成、问答、翻译等。在第3版中，LLaMA使用了包含
128k tokens 的分词器，相较于 LLaMA 2 的 32,000
tokens，词汇表得到了显著扩展。更大的词汇表能够更精确地对语言进行编码，使模型能够更好地理解和生成各种文本内容，从而提高语言表达的丰富度和准确性。模型在
8b 和 70b
尺寸的模型中均采用了分组查询注意力机制，该机制可以在不显著增加计算成本的情况下，提高模型的推理效率，使模型能够更快地生成文本响应，适用于对实时性要求较高的应用场景。

模型对多语言有着良好的支持。模型训练数据涵盖 30
多种语言，使其具备了较强的多语言处理能力，能够为不同语言背景的用户提供服务，并在跨语言的文本生成、翻译等任务中表现出色。作为开源模型，LLaMA
3
为开发者、研究人员和企业提供了一个强大且免费的工具，促进了人工智能技术的广泛应用和创新发展。全球范围内已有超过
30000 个基于 LLaMA开发的新模型。

### 6.5.4 计算机视觉大模型

计算机视觉大模型是一类专为处理和理解视觉数据（如图像、视频等）而设计的先进人工智能模型。早期在计算机视觉领域占据主导地位，通过卷积层、池化层等结构，自动提取图像中的特征，如边缘、纹理、形状等，能够有效处理像素数据和检测层级模式，为图像分类、目标检测等任务提供了基础支持。例如经典的
LeNet、AlexNet、ResNet
等模型，在图像识别领域取得了显著成果。Transformer被引入计算机视觉后，以自注意力机制为核心，能够捕捉图像中的长距离依赖关系，更好地理解图像的全局信息。Vision
Transformer（ViT）是将 Transformer 架构应用于图像识别任务的典型代表。

**1. 生成对抗网络 (Generative Adversarial Networks, GANs)**

传统上，生成问题是通过基于统计的方法来解决的，如玻尔兹曼机，马尔可夫链或变分编码器。尽管它们在数学上很深奥，但生成的样本还远远不够完美。分类模型将高维数据映射到低维数据，而生成模型通常将低维数据映射到高维数据。这两个领域的人们一直在努力改进他们的模型。那么我们能否让这两种不同的模型相互对抗，同时改进自己？如果我们将生成模型的输出作为分类模型的输入，可以用分类模型（矛）来衡量生成模型（盾）的性能。同时，我们可以通过将生成的样本（盾）与真实样本一起输入来改进分类模型（矛）。数据越多通常对机器学习模型的训练越好。两个模型试图战胜对方，提高自身训练过程称为对抗性学习。

如图6-1所示，模型A和B具有完全相反的议程(例如，分类和生成)。然而，在训练的每一步中，模型A的输出都会改进模型B，模型B的输出也会改进模型A：

![A diagram of a process Description automatically
generated](images_6/media/image1.png){width="2.7412959317585304in"
height="1.473581583552056in"}

图6-1 GANs的示意图

GANs正是基于Goodfellow、Pouget-
Abadie、Mirza等人在2014年提出的这个想法设计的。现在，GANs已经成为机器学习中合成音频、文本、图像、视频和3D模型最蓬勃和最流行的方法。本书不会讨论GANs过多的的细节，但是我们会用一个具体的图像生成（图像合成）的例子简单的阐述其主要思想和方法。

GANs的典型结构如图6-2所示。它包含两个不同的网络：生成器网络和鉴别器网络。生成器网络通常将随机噪声作为输入并生成假样本。我们的目标是让假样本尽可能接近真实样本。这就是鉴别器的作用。判别器实际上是一个分类网络，它的工作是判断给定样本是假还是真。生成器试图欺骗和混淆鉴别器以做出错误的决定，而鉴别器则试图区分假样本和真样本。

在此过程中，利用假样本和真样本之间的差异来改进生成器。因此，生成器在生成看起来真实的样本方面变得更好，而鉴别器在分类它们方面变得更好。由于使用真实样本来训练鉴别器，因此训练过程是有监督的。即使生成器总是在不知情下给出假样本，GANs的整体训练仍然是有监督的。

![图示, 示意图
描述已自动生成](images_6/media/image2.png){width="4.310813648293963in"
height="2.6503532370953633in"}

图6-2 一个最基本的GANs网络的示意图

通过让模型学习手写数字MNIST库中图像，最终，模型产生了自己手写数字的图像如图6-3所示。

![图片包含 键盘, 照片, 大, 人们
描述已自动生成](images_6/media/image3.png){width="1.8022714348206474in"
height="3.5978816710411197in"}

图6-3 模型生成的手写数字图像

**2. diffusion扩散模型**

扩散模型（Diffusion
Model）是一类生成式模型，主要用于生成数据，如生成图像、音频等，在计算机视觉和其他数据生成领域应用广泛。它的核心思想是通过对数据分布的逐步学习来生成新的数据样本。

以生成图像为例，扩散模型的目标是从一个简单的分布（如高斯噪声分布）开始，逐步对随机高斯噪声进行去噪，经过一定数量的步骤后，就能得到一个样本，最终生成符合训练数据分布的图像。例如，如果模型是在大量风景图片上训练的，那么最终它能生成新的风景图片。如图6-4所示：

![QR 代码
描述已自动生成](images_6/media/image4.png){width="5.768055555555556in"
height="1.320138888888889in"}

图6-4 扩散模型的示意图[^1]

扩散过程分为正向过程和反向过程。扩散模型的正向过程是一个逐渐添加噪声的过程。假设有一个真实的数据样本，如一张图像。在每一个时间步，会向数据样本添加一定量的高斯噪声。随着时间步的增加，数据样本逐渐被噪声淹没，最终变成一个纯粹的高斯噪声分布。这个正向过程类似于一个马尔可夫链，每一步只依赖于前一步的状态。

反向过程也就是生成过程，是扩散模型用于生成数据的过程，它试图从纯粹的高斯噪声中恢复出符合训练数据分布的样本。在这个过程中，通过多次迭代这个反向过程，从高斯噪声开始，逐步恢复出数据样本。扩散模型的训练目标是最小化预测噪声和真实噪声之间的差异。在训练过程中，会给定一系列带有噪声的数据样本，这些样本是通过正向过程从真实数据样本中生成的。通过大量的训练数据和多次迭代训练，模型学习到如何在反向过程中有效地去除噪声，从而生成符合训练数据分布的样本。

图6-5是用扩散模型生成的一副图像。用的提示语是：\"The leaves of the
Chinese parasol tree in autumn are all yellow.\"

![树上有许多花
描述已自动生成](images_6/media/image5.png){width="3.1421248906386703in"
height="3.1421248906386703in"}

图6-5
根据文本生成图像，使用了stable-diffusion-xl-refiner-1.0预训练大模型

## 6.6 大模型的应用

### 6.6.1 大模型在自然语言处理中的应用

**1. 新闻写作与故事创作**

在当今数字化时代，新闻行业面临着信息快速传播和内容需求多样化的挑战，大模型的出现为新闻写作带来了新的变革。许多新闻机构已经开始运用大模型来自动生成新闻稿件，尤其是在一些数据驱动的领域，如财经、体育、科技等。以财经新闻为例，大模型可以实时监控股票市场的数据变化，包括股价波动、成交量、公司财报等信息，然后根据预设的新闻模板和语言风格，快速生成关于某公司股价走势、市场动态分析等新闻稿件，其速度远远超过传统的记者撰写方式，能够在第一时间为投资者提供最新的市场信息。

在故事创作方面，大模型同样展现出了惊人的潜力。一些在线文学平台利用大模型为创作者提供创意启发和故事框架构建的辅助工具。创作者只需输入一些关键元素，如故事背景、人物设定、主题等，大模型就能基于海量的文学作品数据，生成一个初步的故事大纲，包括情节发展、人物关系的演变等，创作者可以在此基础上进行进一步的创作和润色，大大提高了创作效率，也为一些缺乏灵感的创作者提供了新的思路和方向，激发了更多新颖独特的故事诞生，丰富了文学创作的生态。

**2. 文本摘要自动生成**

随着信息的爆炸式增长，人们每天需要处理大量的文本信息，文本摘要自动生成技术的重要性日益凸显，大模型在这一领域发挥着关键作用。在科研领域，研究人员需要快速了解大量文献的核心内容，大模型能够对科研文献进行高效的摘要生成。例如，对于一篇复杂的生物学研究论文，大模型可以通过对论文中的实验数据、研究方法、结论等关键部分的分析，提取出主要的研究成果和创新点，生成简洁明了的摘要，帮助研究人员在短时间内判断该文献是否与自己的研究方向相关，是否值得深入阅读，从而大大提高了文献筛选的效率，节省了科研人员的宝贵时间，使得他们能够将更多的精力投入到创新性的研究工作中。

在商业领域，企业需要对市场调研报告、竞争对手分析报告等大量文本资料进行快速处理和决策参考，大模型生成的文本摘要能够准确提炼出关键信息，如市场趋势、潜在机会、风险因素等，为企业管理层提供清晰、简洁的决策依据，助力企业在激烈的市场竞争中迅速做出明智的决策，把握市场机遇，应对各种挑战，提升企业的竞争力和市场应变能力。

**3. 机器翻译与问答系统**

在机器翻译领域，大模型的出现带来了质的飞跃，显著提升了翻译的质量和效率，使得跨语言交流更加流畅和自然。例如，谷歌的神经网络机器翻译系统，涵盖了全球多种语言的丰富文本数据，包括但不限于新闻、小说、学术论文、社交媒体等领域的内容。通过对这些海量数据的学习，模型能够捕捉到不同语言之间的语义、语法、词汇用法、文化背景等多方面的复杂关系和细微差异，从而在翻译过程中能够更加准确地将源语言的含义转化为目标语言的自然表达。在实际应用中，对于一些复杂的句子结构和具有文化内涵的词汇，大模型的优势尤为明显。例如，英语中的习语"kick
the
bucket"，字面意思为"踢水桶"，但实际含义是"去世"，谷歌的神经网络机器翻译系统能够利用其对大量英语文本的学习，准确理解该习语的隐喻意义，并在翻译成其他语言（如中文）时，给出符合中文表达习惯的"去世"这一译文，而不是简单地逐字翻译，从而避免了翻译中的歧义。

**4. 智能问答系统的应用**

在客服领域，许多企业纷纷采用基于大模型的智能客服系统来应对客户的咨询和问题解答。以苹果公司为例，其Siri智能助手集成了先进的大语言模型，能够理解用户提出的各种关于苹果产品的问题，如iPhone的功能操作、故障排除、软件使用方法等，并快速给出准确、详细的解答。当用户询问"Siri，我的iPhone屏幕突然变暗了怎么办？"时，Siri能够迅速分析问题，结合其预训练的知识和对常见iPhone问题的学习，给出诸如"您可以尝试调整屏幕亮度设置，或者检查是否开启了自动亮度调节功能。如果问题仍然存在，可能是由于电池电量低或系统故障导致，建议您充电后重启手机"等实用的解决方案，大大提高了客户服务的效率和质量，减轻了人工客服的工作压力，同时也为用户提供了更加便捷、即时的技术支持，提升了用户对企业产品的满意度和使用体验。

在智能助手方面，除了苹果的Siri，亚马逊的Alexa、谷歌的Assistant等也都依托强大的大模型技术，具备了更加智能、灵活的问答能力，不仅能够回答日常生活中的常识性问题，如天气查询、路线导航、新闻资讯获取等，还能够根据用户的历史提问和使用习惯，提供个性化的服务和建议，如推荐符合用户口味的餐厅、音乐、电影等，真正成为用户生活中的得力助手。

### 6.6.2 大模型在计算机视觉中的应用

**1. 人脸识别技术**

人脸识别技术作为计算机视觉领域的重要研究方向，在安防、金融、交通等众多领域有着广泛的应用。大模型的出现为人脸识别技术带来了显著的提升，尤其是在提高识别准确率和鲁棒性方面发挥了关键作用。在安防领域，人脸识别技术被广泛应用于门禁系统、视频监控等场景，对于保障公共安全具有重要意义。传统的人脸识别模型在面对复杂环境下的人脸图像时，如光照条件变化、人脸姿态变化、遮挡等情况，往往会出现识别准确率下降的问题。而大模型通过学习海量的人脸数据，能够更好地捕捉人脸的关键特征和细微差异，从而有效应对这些挑战，提高人脸识别的准确性和可靠性。

以某城市的安防监控系统为例，该系统采用了基于大模型的人脸识别技术，对城市中的重要公共场所进行实时监控。在实际应用中，大模型能够准确识别出不同光照条件下的人脸，无论是在强光直射还是在阴影区域，都能保持较高的识别准确率。同时，对于人脸姿态的变化，如侧脸、低头、抬头等情况，大模型也能通过对多角度人脸数据的学习，准确地进行识别和匹配。此外，当人脸存在部分遮挡时，如戴口罩、帽子等，大模型依然能够凭借其强大的特征提取能力，从遮挡部分较少的区域提取关键特征，实现准确的身份识别，大大提高了安防系统的智能化水平和安全性。

**2. 物体识别与场景理解**

大模型在物体识别和场景理解方面也取得了重要突破，为自动驾驶、智能监控、智能仓储等领域的发展提供了有力支持。在自动驾驶领域，物体识别和场景理解是实现安全驾驶的关键技术之一。大模型能够对车辆行驶过程中的各种物体进行快速、准确的识别，包括行人、车辆、交通标志、信号灯等，并对整个行驶场景进行实时的理解和分析，从而为自动驾驶系统的决策提供重要依据。通过对大量的道路场景数据进行学习，车辆能够在复杂的交通环境中准确地识别出不同类型的车辆和行人，并根据它们的位置、速度、运动方向等信息，实时调整车速、转向等操作，实现安全、高效的自动驾驶。在遇到交通信号灯时，大模型能够快速准确地识别信号灯的状态，并根据交通规则做出相应的决策，如停车、启动、减速等。

在智能监控领域，大模型可以对监控视频中的物体和场景进行智能分析，实现异常行为的自动检测和预警。例如，在机场、火车站等公共场所的监控系统中，大模型能够实时识别出人群中的异常行为，如奔跑、打斗、徘徊等，并及时发出警报，通知安保人员进行处理，提高了公共场所的安全性和管理效率。

在智能仓储领域，大模型可以实现对货物的自动识别和分类，提高仓储管理的智能化水平。通过对货物图像的学习，大模型能够准确识别出不同种类的货物，并根据其属性和存储要求，自动规划存储位置和搬运路径，实现仓储作业的自动化和智能化。

### 6.6.3 大模型在其他领域的应用

**1. 疾病诊断与预测**

在医疗健康领域，大模型能够对X光、CT、MRI等各类医学影像进行精准分析，辅助医生快速、准确地发现病变部位和特征，从而提高诊断的准确性和效率。在肺部疾病诊断中，大模型可以通过对大量肺部CT影像数据的学习，识别出肺结节、肺炎、肺癌等疾病的典型影像特征，包括结节的大小、形状、密度、边缘特征以及与周围组织的关系等。对于肺结节的良恶性判断，传统的诊断方法往往依赖于医生的经验和主观判断，具有一定的局限性，而大模型能够综合分析多个影像特征以及患者的临床信息，如年龄、性别、吸烟史、家族病史等，提供更加客观、准确的诊断建议，帮助医生更早期地发现潜在的恶性病变，提高患者的生存率和治愈率。

在心血管疾病领域，大模型可以通过分析心脏超声、冠状动脉造影等影像数据，预测心血管疾病的发生风险和疾病进展情况。例如，通过对心脏结构和功能指标的分析，结合患者的血压、血脂、血糖等生理参数以及生活方式因素，大模型能够预测患者未来发生心肌梗死、心力衰竭等心血管事件的概率，为患者提供个性化的预防和治疗方案，提前采取干预措施，降低心血管疾病的发病风险和死亡率。

此外，大模型还可以对疾病的发展趋势进行预测，如癌症患者的生存期预测、慢性疾病的病情恶化预测等，帮助医生制定更加合理的治疗计划和随访策略，优化医疗资源的分配，提高医疗服务的质量和效益。

**2. 药物研发**

在药物靶点发现阶段，大模型能够整合海量的生物医学数据，包括基因组学、蛋白质组学、代谢组学等多组学数据，以及大量的医学文献和临床试验数据，通过深度学习算法挖掘潜在的药物靶点。在癌症治疗中，大模型可以分析癌细胞的基因表达谱、蛋白质相互作用网络等信息，识别出与癌症发生、发展密切相关的关键基因和蛋白质，这些潜在的靶点为后续的药物设计和开发提供了重要的方向和基础。

在药物生产过程中，大模型可以根据已确定的药物靶点结构，利用分子模拟和虚拟筛选技术，快速筛选出具有潜在活性的化合物。通过对化合物的结构、活性、毒性等多方面的预测和评估，大大缩短了新药研发的周期，降低了研发成本。利用大模型对化合物库进行虚拟筛选，能够在短时间内从数百万甚至数十亿个化合物中筛选出少数具有高活性和低毒性的候选药物，然后再进行进一步的实验验证和优化，提高了药物研发的成功率和效率。

**3. 个性化医疗**

个性化医疗是大模型在医疗领域的另一个重要应用方向。随着基因测序技术的快速发展，越来越多的患者能够获得自己的基因组信息。大模型可以根据患者的基因组数据、临床症状、病史以及其他相关的生理和病理指标，为患者量身定制个性化的治疗方案。在癌症治疗中，不同患者的肿瘤细胞可能具有不同的基因突变和生物学特性，大模型能够分析这些个体差异，预测患者对不同化疗药物、靶向药物或免疫治疗药物的反应和疗效，从而选择最适合患者的治疗药物和剂量，提高治疗效果，减少药物不良反应的发生。

**4. 风险评估与投资决策**

在金融领域，风险评估与投资决策是至关重要的环节，大模型的应用为其带来了新的方法和思路。传统的信用风险评估方法往往依赖于有限的历史数据和简单的统计模型，难以全面、准确地评估客户的信用状况。而大模型能够整合多源数据，包括客户的基本信息、信贷记录、消费行为、社交网络数据等，进行深度分析，从而更精准地预测客户的违约风险。

通过对大量历史客户数据的学习，大模型可以识别出不同特征与违约风险之间的复杂关系，例如客户的职业稳定性、收入波动情况、近期消费模式的变化以及社交网络中的信用关联等因素对其还款能力和意愿的影响。在评估一位个体经营者的信用风险时，大模型不仅会考虑其企业的经营状况、财务报表等传统信息，还会分析其在社交媒体上的商业活动痕迹、与合作伙伴的互动情况以及所在行业的整体趋势等非传统数据，从而更全面地了解其信用状况，为银行提供更准确的风险评估结果。

**5. 市场趋势预测与交易策略**

大模型在金融市场的数据分析、趋势预测和交易策略制定中也发挥着重要作用。金融市场数据具有高度的复杂性、波动性和噪声，传统的分析方法难以捕捉到其中的潜在规律和趋势。大模型凭借其强大的学习能力和对海量数据的处理能力，能够对市场数据进行全面、深入的挖掘和分析。

在股票市场中，大模型可以整合宏观经济数据、公司财务报表、行业动态、新闻舆情、社交媒体情绪等多维度信息，对股票价格的走势进行预测。通过对历史数据的学习，大模型能够发现不同因素之间的关联模式，例如宏观经济政策的调整如何影响特定行业的股票表现，公司的重大事件（如新产品发布、管理层变动）如何在股价上得到反映，以及市场情绪的变化如何引发股价的短期波动等。

基于这些预测结果，投资者可以制定更加科学、合理的交易策略，如资产配置、择时交易、风险对冲等。在市场趋势向上时，大模型可以建议投资者适当增加股票资产的配置比例；而在市场出现不稳定因素或负面情绪时，模型可以提醒投资者及时调整投资组合，降低风险。一些量化投资机构利用大模型构建交易策略，通过对大量历史交易数据和市场数据的回测和优化，不断改进交易模型，提高投资收益，在复杂多变的金融市场中获取竞争优势，推动了金融投资领域向更加智能化、精准化的方向发展。

## 6.7 人工智能与大数据

数据正在以越来越快的速度不断增长[^2]。随着数据化运营所带来的利好日益明显，越来越多的机构开始重视海量数据背后蕴藏的无限"财富"，包括中国、美国等许多国家都将数据资源的开发提高到了战略的高度，纷纷出台大数据战略。由此，数据产业在近年来得到了迅速的发展。

这些不断增长的庞大数据中包含着各种丰富的信息，使得从大数据中提取有价值信息的研究成为了一件非常有趣的工作。例如，可以通过收集一个人的日常数据，来预测他的喜好或者他下一步要做什么（你最直接的感受可能是：当在网上购买了某件用品之后，这个网址接着就会向你推荐类似的其它商品）。

数据的大容量和动态性为改变商业、政府、科学和人们的日常生活带来了可能，但对这些庞大数据的存储和管理不是一件很容易的事情，而要从海量数据中提取有价值的信息，需要各种数据分析技术，更是一件非常具有挑战的工作。

### 6.7.1 什么是大数据？

数据科学家维克托·迈尔-舍恩伯格（Viktor
Mayer-Schönberger）在他编写的《大数据时代》中指出：大数据（Big
Data）是那些在小规模数据基础上无法完成而只有在大规模数据基础上才能够做到的事情。

这个话听上去有点不好理解，举个例子。比如：你希望假期乘飞机出去度假旅行，按照一般的概念，你认为越早订票，票价会越便宜，所以你在确定好度假计划后第一时间就订了机票。但上飞机后，你突然发现左右邻座的机票都比你买的机票价格低，是不是顿时就会有一种不舒服的感觉？

知名的人工智能研究人员奥伦·埃齐奥尼（Oren
Etzioni）也碰到了类似的事情，为此他开始研究利用数据来预测飞机票的价格变化趋势。他收集了近十万亿条价格记录来预测美国国内航班的票价，预测准确度达到75％，从而使利用这个预测系统购买机票的旅客，平均每张机票节省了50美元。

这个例子想说明的是：大数据并不是一个完全确切的概念。研究大数据的原因是因为要处理的数据越来越多，超出了一般个人计算机所具有的内存容量，从而不得不研究一些新的数据处理技术，如谷歌（Google）的MapReduce和开源Hadoop平台。这些技术使我们能够处理的数据量大大增加。

全球最著名的管理咨询公司麦肯锡（McKinsey&Company）对大数据的定义是：大数据是具有大规模、分布式、多样性和/或时效性的数据。一般来讲，大数据应具有如下显著特征：

（1）数据体量巨大**（Volume）**。大数据的数据量远不止成千上万行，而是动辄几十亿行，数百万列。其起始计量单位提出是PB（Petabyte，1PB=1024TB）、EB（Exabyte，1EB=1024PB）、甚或ZB（Zettabyte，1ZB=1024EB）、YB（Yottabyte，1YB=1024ZB）。

（2）数据类型繁多（Variety）。数据的来源多样，数据类型和结构复杂（Complexity），涉及各种各样数据源、数据格式和数据结构。如网络日志、音频、视频、图片、地理位置信息等。多种不同类型的数据对数据的处理能力提出了更高的要求。

（3）**价值密度低（Value）。大数据的**第三个特征是数据价值密度相对较低，或者说是浪里淘沙却又弥足珍贵。随着互联网以及物联网的广泛应用，信息感知无处不在，信息海量，但价值密度较低，如何结合业务逻辑并通过强大的机器算法来挖掘数据价值，是大数据时代最需要解决的问题。

（4）新数据的创建和增长速度快（**Velocity**）。大数据的第4个特征是数据增长速度和处理速度快，时效性要求高。比如搜索引擎要求几分钟前的新闻能够被用户查询到，个性化推荐算法尽可能要求实时完成推荐。

这就是大数据的所谓4V（**Volume，**Variety，**Value，Velocity**）特征。研究大数据的目的是为了从这些海量数据中发掘出有价值的信息（如从大量机票售价记录中发掘票价变化走势）。

### 6.7.2 数据的组织形式

如今，能够产生大数据的源头很多。比如我们每天使用的手机（你的位置信息、上网信息、在微信等社交网站上发的贴等）、满布城市各个角落的监控视频、医学影像、人类基因测序等。图6-6给出了海量数据的几个主要来源。

![](images_6/media/image6.emf)

图6-6 大数据的主要来源

这些不同来源的数据有着不同的形式，包括结构化数据和例如财务数据、文本文件、多媒体文件等的非结构化数据，还有部分大数据属于半结构化数据。

**1. 结构化数据**

结构化数据是指有固定格式和有限长度的数据，以行为单位，一行表示一个实体（一个具体的对象），每一行数据都有相同的属性。例如我们经常填的表格就是结构化数据。所以，结构化数据是由二维表结构来表达和实现的数据，它严格遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。图6-7所示是一个关于学生信息的典型结构化数据示例：

  ------------------------------------------------------------------------
      StudentID        StudentName      StudentGender       StudentAge
  ----------------- ----------------- ------------------ -----------------
      201700110           张琳                女                18

      201700111          李一平               男                19
  ------------------------------------------------------------------------

图6-7 结构化数据例

**2. 非结构化数据**

顾名思义，非结构化数据就是没有固定结构的数据，没有预定义的数据模型，难以用二维逻辑表来表示。现在越来越多的数据是非结构化数据。例如网页，有时候非常长，有时候又非常短，长度和格式都不固定。各种文本文件、图片、视频、音频等都属于非结构化数据。随着网络技术的发展，未来80%-90%的增长数据会属于非结构化数据类型。

非结构化数据的管理总体上可以包括文件管理、特定的数据库管理（如NoSQL数据库）[^3]、通过索引来查找的搜索引擎管理。对非结构化数据，通常需要先进行一定的清理、整合、分类、索引等策略来提高数据管理效率和准确性。

**3. 半结构化的数据**

半结构化数据是结构化数据的一种形式，它不符合如二维表格或关系数据库形式关联起来的数据模型结构，但包含相关标记（如XML），可以用来分隔语义元素以及对记录和字段进行分层。因此，它也被称为自描述的半结构化数据。

本结构化数据可以用树或图的数据结构存储。例如在以下的xml文件中，\<person\>标签是树的根节点，\<stuid\>、\<name\>、\<age\>和\<gender\>标签是子节点。通过这样的数据格式，可以自由地表达很多有用的信息，包括自我描述信息。所以，半结构化数据具有很好的扩展性。

\<person\>

\<stuid\>20\</stuid\>

\<name\>A\</name\>

\<age\>13\</age\>

\<gender\>female\</gender\>

\</person\>

虽然从结构上可以将数据分为这三种类型，但不断增长的大部分数据都可以说是混合类型。例如我们上网时产生的鼠标点击流数据，既包含有非结构化的原始日志（LOG），也有如浏览器的类型信息等取值有一定规律的结构化数据；又如某个典型的二维表格中可能存储有XML格式的文件等。

 
目前，对结构化数据的存储、分析技术已非常成熟。但大数据的增长越来越非结构化，对这类数据的分析还需要不同的技术来应对。

### 6.7.3 数据分析的生命周期

大量的数据，如果仅仅"存着"是没有什么意义的，只有想办法将这些数据"用"起来，数据才变得有价值。那么，数据怎么样才能对人有用呢？举个例子：世界各地的证券交易所中，每天都会产生大量的股票交易数据。这些交易数据经过整理，以图的形式（交易曲线）显示出来，为关心股票行情的人提供各类股票涨跌的信息。对绝大多数人来讲，看这些曲线只是了解一下各种股票的变化情况。但对一个数据科学家，他可能会更关注如何从大量的交易信息中发掘出某种规律，从而去判断（预测）股票未来的涨落。从信息中提取出的规律，可以称为知识。有了这些知识，就可以将其用于实战了。例如，通过对大量历史交易数据的分析，就可能预测出股票未来的涨跌趋势，这样就可以赚钱了。

从这个例子中可以看出，要让数据变的有用，首先需要尽可能收集到更多的数据（试想只看到一次股票交易数据，如何能判断其下一次是涨还是跌？）；然后，对收集到的结构化或非结构化数据要进行整理；再利用各种方法从中提取出潜在的知识。图6-8给出了数据分析的主要环节。

![](images_6/media/image7.emf)

图6-8 数据分析主要环节

**1. 数据收集与存储**

收集数据并不是一件很容易的事情。在许多IT（Information
Technology）企业中，数据是集中存放在业务性数据库中并有专人管理。进行数据分析首先需要获得访问这些数据的权限，同时，为了不影响生产业务，数据分析往往需要将数据从业务性数据库中提取出来，进行加工整理，再导入到分析性数据库（数据仓库）。这个过程称为ETL
（Extract、Transform、Load）。

在有些情况下，数据可能并没有现成存在，需要用专门的技术进行采集和存储。收集数据的方法一般有两种。一种方法是利用各种工具从网上爬取。这类从网上爬取数据的工具称为**网络爬虫**。它是一种按照一定规则、自动抓取万维网信息的程序。例如我们熟悉的搜索引擎就是这样一种程序。搜索引擎首先从网上抓取大量的信息下载到它自己的数据中心，并对数据进行一定的组织和处理。然后再根据你给出的搜索查询，按一定的顺序将查询结果反馈给你。

另一种收集数据的方法是推送。有很多种传感设备可以记录数据并将收集到的数据推送的数据中心。例如电子手环，可以将记录的你每天跑步的数据、心跳数据、睡眠数据等都上传到它自己的数据中心；你每天在各个网站上浏览的网页信息也都存放在网站的日志数据库中。

数据正在变得越来越重要，购物网站能够知道你想买什么就是因为它有你的历史交易数据。这也就是为什么要研究数据管理的原因。

**2. 数据预处理**

数据分析领域中的数据预处理也称为数据清洗和整理。虽然数据是一笔巨大的财富，但无论是通过网络爬取还是各种传感设备推送的数据都称为原始数据。原始数据大多杂乱无章，且来自多个不同的数据源。如：不同的历史数据、不同的模式和不同的格式约定等。要对这些数据进行分析以挖掘出有价值的信息，首先需要将这些数据整理成一个单一、一致、高质量的数据集。

考虑一个由各种医院、诊所和药店组成的医疗机构，患者的各类信息由多个系统存储，包括人口统计数据、实验室数据、索赔数据和药房数据等。医生、护士和药剂师等在输人各自的数据时不能保证数据一致且没有人为错误。事实上，由于可能存在的简单的数据输入错误，使各种基准值，包括实验室结果、血压、体重指数等，都不再具有有效的临床价值。这样的"脏数据"将会极大影响数据分析的有效性。因此，必须要在数据分析之前花费一定时间来探索数据的质量，并应用各种技术清洗和规范其输入数据集。

**3. 特征提取**

数据分析的目的是从大量数据中发掘出有价值的知识，并为未来的决策提供支撑。换句话讲，就是基于对已发生的历史数据的分析，去预测正在进行的同类事件未来可能的结果。这些工作需要借助各种分析模型和算法。所有的模型或算法都需要输入数据，为了确保分析结果的有效性，必须考虑输入的数据具有哪些特征以及这些特征是否与分析或预测的结果（输出）存在关联性，亦即需要选择出最适合任务的变量或数据特征。有时这些特征已经存在于数据集中；有时需要通过组合多个现有特征来创建或设计出新的特征。

例如，某移动通信运营商准备建立客户流失模型。该运营商的数据库中已收集有过去6个月内所有客户呼叫的完整日志数据，数据集中包含客户编号、客户使用该平台的起始日期、当前使用的移动设备、月平均通信时长、付款计划和邮政编码等信息变量。这些变量可以作为客户流失预测模型的输入特征，也可以从中派生出一些复杂特征，或者对这些变量进行一些复杂计算产生新特征。例如可以从这些原始变量中提取出"最后一个月的电话呼出次数"待征或"某客户通话规律"特征等。

从数据中寻找特征的工作称为特征工程，通常会涉及到统计学、信息理论、可视化和文本分析等各种技术，是数据科学中最困难和最重要的任务之一。提取哪些特征以及提取多少特征，核心原则就是这些特征会使你构建的特定模型预测效果最好。

**4. 数据挖掘与数据分析**

从数据中发现各种知识的技术有个专门的名词，叫做**数据挖掘**（Data
Mining），它和数据分析有很多相似之处，只是研究的侧重点和实现手法有一些区分。

（1）相比于数据挖掘，数据分析更针对的是体量巨大的数据。因此，分析时不可能用单台机器完成而是需要多台计算机同时运算，也就是所谓的分布式运算。

（2）数据挖掘一般通过自己编写程序来实现，而数据分析更多的是借助现有的分析工具进行。

（3）数据分析要求对所从事的行业有比较深的了解和理解，并且能够将数据与自身的业务紧密结合起来；而数据挖掘不需要有太多的行业专业知识。

（4）数据挖掘更多注重于技术（算法）层面的结合以及数学和计算机的集合，数据分析需要结合统计学、营销学、心理学以及金融、政治等方面进行综合分析。

数据挖掘和数据分析都是从数据中提取有价值的知识，它们的界限正在变得越来越模糊，很多数据分析人员开始使用编程工具进行数据分析，而数据挖掘人员在结果表达及分析方面也会借助数据分析的手段。事实上，数据挖掘是大数据分析的理论基础，所采用的算法模型都很类似。上述这些区别的主要根源就是大数据分析的体量和多样性。大数据分析主要需解决的是海量数据在多台机器上的分布式存储和并行处理，其基本原理还是数据挖掘技术。考虑到本书并不是大数据分析的专业教材，以下将不再对这两个名词做区分描述，而统一称为数据分析。

Google首席经济学家Hal
Varian曾说过，数据是广泛可用的，所缺乏的是从中提取出知识的能力。数据收集和整理的根本目的是为从海量数据中提取出有价值信息奠定基础。已有很多学者对从数据中提取知识的方法进行研究，研究出了各种有效的数据分析算法。沃尔玛超市从大量用户购买行为数据中，发现男性顾客在购买尿布时常常会同时购买啤酒。由此他们调整了柜台的摆放，将啤酒和尿布的柜台放置在相邻位置，从而提升了这两种产品的销售量。从大量购物数据中发掘出这两种看上去完全无关的货物之间所存在的相互关系的方法，就是数据挖掘技术中非常经典的关联分析。

无论是数据挖掘还是数据分析，核心就是模型构建。这里的模型是一种泛指，一个模型是对现实的一种抽象。构建模型是人们通过一组规则和条件来模仿所观察到的真实场景中发生的事件行为。这些规则和条件通常分为若干类技术，例如关联分析、分类、聚类、回归等。

数据分析的核心工作就是选择一种或一系列适合于特定研究的候选基础模型或分析技术，将其应用于获取的数据集，通过训练、测试和验证，构建出针对特定问题的数据分析模型，并将分析结果以可视化形式展示给用户。

### 6.7.4 人工智能拥抱大数据

一个神经网络的设计除了结构（如：隐藏层的层数，每个隐藏层包含的神经元个数）设计之外，还需要对神经网络中的各种参数（如：神经元之间的连接权重，激活函数系数等）进行学习优化（调参），也就是模型训练。参数调整的工作量复杂且非常庞大，需要的计算量会随着网络节点和输入数据的规模变得非常巨大，如果在个人计算机上进行模型训练，可能做一轮训练就需要花费几天时间，甚至在可接受的时间内无法完成。特别是对隐藏层超过3层的深度神经网络更是如此。这就需要用到大数据平台了。大数据平台可以汇聚多台机器的力量一起来计算，以实现在有限时间内得到想要的结果。

**1. 大数据需要云计算**

微型计算机是我们日常接触最多，事实上也可能是惟一使用的通用计算设备，它完全能够满足一般的数据处理、数据分析。例如对一个年级的学生的网络在线课程学习行为分析、一个小型选课系统的管理等。但是，当处理的数据量达到一定量级时（如PB、EB等），一台个人计算机或服务器就完全不能满足要求，这个时候就用到云计算了。

大数据的体量巨大，为了实现对大量数据的处理，有一种方法当然是自己拥有足够多的计算设备。例如：某公司可以购置1000台计算机做定期财务状况分析。但由于财务状况分析1个月才做一次，这1000台计算机1个月才使用一次，造成很多的浪费。

如果能够仅在需要的时候拥有这1000台机器，用完后再还回去，它们就可以去做其他事情了。这不仅可以实现资源的有效利用，还可以省很多钱。这正好就是云计算的特点。

云计算利用虚拟化技术将众多计算设备"包裹"在一起，可以为大数据的处理提供灵活的资源，同时也可以将大数据平台（如Hadoop）部署在"云"上。现在主要的公有云平台上基本都有大数据解决方案。对上述需要定期做财务状况分析的公司，不需要再购买1000台机器，只要到公有云上选择一下，就可以调用1000台已经部署好大数据分析平台的机器，公司只需要把待分析的数据放进去运算就可以了。由此，大数据和云计算就自然而然地联系在了一起。

**2. 人工智能需要大数据**

人工智能的发展和应用高度依赖于大数据。没有大量数据的支持，人工智能的许多应用都将受到限制和制约，特别是当前迅速发展的生成式人工智能（Generative
Artificial
Intelligence，GAI）技术，更是需要通过对大量数据的学习和训练，才能使模型够自动捕捉数据中的规律和特征，从而生成与输入数据相似或全新的数据。如AlphaGo，AlphaFold
和
ChatGPT等这些我们耳熟能详的产品，都表现出了较强的内容生成能力（即"无中生有"能力）。

AlphaGo利用数百万人类围棋专家的棋谱，经过对大量围棋对弈数据进行训练，实现根据当前落子局势，从已有落子的学习中合成一个策略，以更好应对当前落子；AlphaFold从蛋白质的基因序列和其三维空间结构的配对数据中进行学习，可以实现按照给定的基因序列输入，合成一个刻画生命功能的蛋白质三维结构；ChatGPT是一个复杂的神经网络大语言模型（Large
Language
Model，LLM），按照"共生则关联"的原理，挖掘出句子中单词间的共生概率知识，以机器智能实现统计关联下的语言合成，推动了人工智能由识人辨物和预测决策等向内容合成跃升，即人工智能内容合成（
Artificial Intelligence Generated Content , AIGC )。

训练数据集大小和训练参数数量是衡量模型的性能和训练效果的关键指标。著名的**大语言模型ChatGPT4的**训练基于一个包含13万亿个token（token代表了一种基本单元或标识符，在LLM中，1个token可以是一个单词、字母、数字、标点符号等）的庞大数据集，涵盖互联网上的大量网页、书籍、新闻报道等多种类型的数据。**使用了1.8万亿个训练参数。**

**3. 大数据、云技术、人工智能**

人工智能可以做的事情很多。如：垃圾邮件鉴别，黄色暴力文字和图片鉴别，机器翻译，语音识别与合成等。不同的应用可以有不同的算法，但无论哪种算法，实现人工智能要依赖大量数据，如果没有数据，就算有人工智能算法也无能为力。

大量的数据需要存储和处理，这就用到了云技术。对积累了大量数据的大数据公司，需要利用人工智能算法来提供服务。而人工智能算法既需要大量的数据，也需要存储了大量数据的云计算平台来完成模型的训练。这样，云计算、大数据、人工智能就整合在了一起，彼此相关、相互依存。而无论是数据的采集、存储、或分析处理，都离不了计算系统。

## 习题

一. 填空题

1\. 在AI的历史上，通常认为（ ）年是人工智能诞生的元年。

2\. 由（ ）在1950年提出的（
）测试是判定计算机是否会有像人类一样的行为的最好的测试方案。

3\. 大模型的核心技术包括深度学习、（ ）机制和参数优化算法。

4\. 扩散模型的反向过程是从（ ）中恢复出符合训练数据分布的样本。

5\. 在数据预处理中，（
）是为数据赋予特定标签或注释，以便模型学习语义信息。

6\. 数据预处理环节中，数据清洗的目的在于去除数据中的（ ）的部分。

7\. 文心一言的基础架构是 Transformer 架构，并在此基础上进行了（ ）。

二. 选择题

1\. 以下不属于人工智能伦理问题的是（ ）

A. 自动驾驶事故责任判定 B. 智能机器人情感权利争议

C. 数据隐私泄露风险 D. 算法效率优化

2\. 以下不属于AI的研究领域的是：

A. 模式识别 B. 逻辑学 C. 图论 D. 自然语言处理

3\. 以下哪个问题不太可能由AI的研究引发？

A. 技术失控问题 B. 道德伦理问题 C. 环境健康问题 D. 就业问题

4\. 以下属于确定性模型的是（ ）

A. 天气预测模型 B. 圆面积计算模型

C. 股票价格预测模型 D. 语音识别模型

5\. 大模型的 "涌现能力" 指（ ）

A. 模型参数突破一定规模后突然具备的复杂能力

B. 数据量增加导致模型性能线性提升

C. 硬件升级带来的计算效率提升

D. 多模态数据融合产生的新功能

6\. 大模型发展的萌芽期是以（ ）为代表的传统神经网络模型阶段。

A. RNN B. CNN C. Transformer D. GAN

7\. 以下关于GPT - 4的说法，错误的是（ ）。

A. 基于Transformer架构，核心是自注意力机制 B.
由多个混合专家模型组成，共有16个专家模型 C. 拥有约1.8万亿个参数，是GPT -
3的10倍以上 D. 不具备多模态能力

8\.
扩散模型的核心思想是通过对数据分布的逐步学习来生成新的数据样本，其正向过程是（
）。

A. 一个逐渐去除噪声的过程 B. 一个逐渐添加噪声的过程C.
直接从训练数据生成样本的过程 D. 以上都不对

9\. 在人脸识别技术中，大模型相比传统模型的优势不包括（ ）。

A. 提高识别准确率 B. 降低计算成本 C. 更好地应对复杂环境 D. 提高鲁棒性

10\. 在药物研发过程中，大模型不能在以下哪个阶段发挥作用（ ）。

A. 药物靶点发现 B. 药物临床试验 C. 药物生产过程中的化合物筛选 D.
预测药物的活性和毒性

三. 简答题

1.  人工智能的发展中，第一次遇到的致命问题是什么？后来是如何解决的？

2.  谈谈人工智能将对社会产生什么影响？

3.  为什么说数据预处理是大数据分析的关键环节？

[^1]: 该图像引自https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusion-process.png

[^2]: ^\[1\]^ 据不完全统计，全世界每天会有2500亿字节数据产生。

[^3]: ^\[2\]^
    NoSQL数据库是一种非关系型数据库，具有管理非结构化数据的功能，主要用于解决大规模数据集合和多重数据种类带来的挑战。
